import os
import re
import time
import random
import hashlib
import requests
import ssl
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.ssl_ import create_urllib3_context

# ============ é…ç½®åŒºåŸŸ ============
BASE_URL = "https://fabiaoqing.com"
SAVE_DIR = "./emoji_images"  # ä¿®æ”¹æ­¤è·¯å¾„æ¥æ›´æ”¹ä¿å­˜ä½ç½®
MAX_PAGES = 5  # æ¯ä¸ªåˆ†ç±»çˆ¬å–çš„é¡µæ•°
MAX_IMAGES = 100  # æœ€å¤§ä¸‹è½½å›¾ç‰‡æ•°
DELAY_RANGE = (0.5, 1.5)  # è¯·æ±‚å»¶è¿ŸèŒƒå›´(ç§’)
THREADS = 5  # å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°
# ==================================

# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Referer': BASE_URL,
}

class TLSAdapter(HTTPAdapter):
    """è‡ªå®šä¹‰é€‚é…å™¨ï¼Œå¼ºåˆ¶ä½¿ç”¨ç‰¹å®šçš„ TLS ç‰ˆæœ¬å’Œå¯†ç å­¦å¥—ä»¶"""
    def init_poolmanager(self, *args, **kwargs):
        # ä½¿ç”¨ ssl.create_default_context() åŒ¹é… test_urllib3.py çš„æˆåŠŸé…ç½®
        context = ssl.create_default_context()
        context.set_ciphers('DEFAULT@SECLEVEL=1')
        context.minimum_version = ssl.TLSVersion.TLSv1_2
        context.maximum_version = ssl.TLSVersion.TLSv1_2
        # è·³è¿‡è¯ä¹¦éªŒè¯ï¼ˆå¦‚ verify=False æ—¶æ‰€éœ€ï¼‰
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        kwargs['ssl_context'] = context
        return super(TLSAdapter, self).init_poolmanager(*args, **kwargs)

    def proxy_manager_for(self, *args, **kwargs):
        context = ssl.create_default_context()
        context.set_ciphers('DEFAULT@SECLEVEL=1')
        context.minimum_version = ssl.TLSVersion.TLSv1_2
        context.maximum_version = ssl.TLSVersion.TLSv1_2
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        kwargs['ssl_context'] = context
        return super(TLSAdapter, self).proxy_manager_for(*args, **kwargs)

def create_session():
    """åˆ›å»ºå¸¦æœ‰ SSL ä¿®å¤å’Œé‡è¯•æœºåˆ¶çš„ä¼šè¯"""
    session = requests.Session()
    session.headers.update(HEADERS)
    session.mount("https://", TLSAdapter())
    session.verify = False  # è·³è¿‡SSLéªŒè¯
    return session

def get_page_content(session, url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        time.sleep(random.uniform(*DELAY_RANGE))
        response = session.get(url, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or 'utf-8'
        return response.text
    except requests.RequestException as e:
        print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None

def parse_emoji_list(html, base_url):
    """è§£æè¡¨æƒ…åŒ…åˆ—è¡¨é¡µé¢ï¼Œæå–å›¾ç‰‡URL"""
    soup = BeautifulSoup(html, 'html.parser')
    images = []
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥åŒ¹é…å›¾ç‰‡
    selectors = [
        'img.ui.image.lazy',  # æ‡’åŠ è½½å›¾ç‰‡
        'img[data-original]',  # data-originalå±æ€§
        '.bqppdiv img',  # è¡¨æƒ…åŒ…å®¹å™¨
        '.tagbqppdiv img',  # æ ‡ç­¾è¡¨æƒ…åŒ…
        '.bqba img',  # å¦ä¸€ç§å®¹å™¨
        'img.lazy',  # æ‡’åŠ è½½ç±»
        '.image img',  # imageç±»å®¹å™¨
        'a.image img',  # é“¾æ¥ä¸­çš„å›¾ç‰‡
    ]
    
    found_imgs = set()
    for selector in selectors:
        for img in soup.select(selector):
            # ä¼˜å…ˆè·å–data-originalï¼ˆæ‡’åŠ è½½çœŸå®åœ°å€ï¼‰
            src = img.get('data-original') or img.get('src') or img.get('data-src')
            if src:
                # è¿‡æ»¤æ‰å ä½å›¾å’Œå›¾æ ‡
                if 'placeholder' in src.lower() or 'icon' in src.lower():
                    continue
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = urljoin(base_url, src)
                # åªä¿ç•™å›¾ç‰‡æ–‡ä»¶
                if any(ext in src.lower() for ext in ['.gif', '.jpg', '.jpeg', '.png', '.webp']):
                    found_imgs.add(src)
    
    images = list(found_imgs)
    print(f"åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
    return images

def parse_detail_page(html, base_url):
    """è§£æè¯¦æƒ…é¡µé¢è·å–é«˜æ¸…å›¾ç‰‡"""
    soup = BeautifulSoup(html, 'html.parser')
    images = []
    
    # è¯¦æƒ…é¡µå¯èƒ½æœ‰æ›´å¤§çš„å›¾ç‰‡
    for img in soup.select('img'):
        src = img.get('data-original') or img.get('src') or img.get('data-src')
        if src:
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(base_url, src)
            if any(ext in src.lower() for ext in ['.gif', '.jpg', '.jpeg', '.png', '.webp']):
                if 'placeholder' not in src.lower():
                    images.append(src)
    
    return images

def get_category_urls(session):
    """è·å–åˆ†ç±»é¡µé¢URLåˆ—è¡¨"""
    urls = [
        f"{BASE_URL}/biaoqing",  # è¡¨æƒ…åŒ…åˆ—è¡¨
        f"{BASE_URL}/zui/hot",   # çƒ­é—¨
        f"{BASE_URL}/zui/new",   # æœ€æ–°
    ]
    
    # å°è¯•è·å–æ›´å¤šåˆ†ç±»
    html = get_page_content(session, BASE_URL)
    if html:
        soup = BeautifulSoup(html, 'html.parser')
        for a in soup.select('a[href*="/search/bqb/"]'):
            href = a.get('href')
            if href:
                if href.startswith('/'):
                    href = urljoin(BASE_URL, href)
                urls.append(href)
    
    return list(set(urls))[:5]  # é™åˆ¶åˆ†ç±»æ•°é‡

def download_image(session, url, save_dir, index):
    """ä¸‹è½½å•å¼ å›¾ç‰‡"""
    try:
        # ä»URLæå–æ–‡ä»¶å
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        # ç¡®ä¿æœ‰æ­£ç¡®çš„æ‰©å±•å
        if not filename or '.' not in filename:
            ext = '.gif' if '.gif' in url.lower() else '.jpg'
            filename = f"emoji_{index:04d}{ext}"
        else:
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
            filename = re.sub(r'[^\w\-.]', '_', filename)
        
        filepath = os.path.join(save_dir, filename)
        
        # ä¸‹è½½å›¾ç‰‡
        response = session.get(url, timeout=15)
        response.raise_for_status()
        
        # ä½¿ç”¨MD5æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹
        content_hash = hashlib.md5(response.content).hexdigest()[:8]
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ hashåç¼€
        if os.path.exists(filepath):
            name, ext = os.path.splitext(filename)
            filepath = os.path.join(save_dir, f"{name}_{content_hash}{ext}")
        
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        size_kb = len(response.content) / 1024
        print(f"âœ“ ä¸‹è½½æˆåŠŸ: {os.path.basename(filepath)} ({size_kb:.1f}KB)")
        return filepath
    except Exception as e:
        print(f"âœ— ä¸‹è½½å¤±è´¥ {url}: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    # ç¦ç”¨SSLè­¦å‘Š
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    print("=" * 50)
    print("  å‘è¡¨æƒ… (fabiaoqing.com) è¡¨æƒ…åŒ…çˆ¬è™«")
    print("=" * 50)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(SAVE_DIR, exist_ok=True)
    print(f"\nğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°: {os.path.abspath(SAVE_DIR)}")
    
    session = create_session()
    all_image_urls = set()
    
    # 1. è·å–åˆ†ç±»URL
    print("\nğŸ” è·å–åˆ†ç±»é¡µé¢...")
    category_urls = get_category_urls(session)
    print(f"æ‰¾åˆ° {len(category_urls)} ä¸ªåˆ†ç±»é¡µé¢")
    
    # 2. éå†åˆ†ç±»é¡µé¢æ”¶é›†å›¾ç‰‡URL
    for cat_url in category_urls:
        print(f"\nğŸ“‚ å¤„ç†åˆ†ç±»: {cat_url}")
        
        for page in range(1, MAX_PAGES + 1):
            if len(all_image_urls) >= MAX_IMAGES:
                break
            
            page_url = f"{cat_url}?page={page}" if page > 1 else cat_url
            print(f"  ç¬¬ {page} é¡µ...", end=" ")
            
            html = get_page_content(session, page_url)
            if not html:
                print("è·å–å¤±è´¥")
                break
            
            images = parse_emoji_list(html, BASE_URL)
            if not images:
                print("æ— æ›´å¤šå›¾ç‰‡")
                break
            
            all_image_urls.update(images)
            print(f"æ‰¾åˆ° {len(images)} å¼ , ç´¯è®¡ {len(all_image_urls)} å¼ ")
        
        if len(all_image_urls) >= MAX_IMAGES:
            print(f"\nå·²è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ ({MAX_IMAGES})")
            break
    
    # 3. å¹¶å‘ä¸‹è½½å›¾ç‰‡
    image_urls = list(all_image_urls)[:MAX_IMAGES]
    print(f"\nâ¬‡ï¸  å¼€å§‹ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡ (å¹¶å‘æ•°: {THREADS})...")
    print("-" * 50)
    
    downloaded = []
    with ThreadPoolExecutor(max_workers=THREADS) as executor:
        futures = {
            executor.submit(download_image, session, url, SAVE_DIR, i): url
            for i, url in enumerate(image_urls, 1)
        }
        for future in as_completed(futures):
            result = future.result()
            if result:
                downloaded.append(result)
    
    # 4. ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 50)
    print(f"âœ… ä¸‹è½½å®Œæˆ!")
    print(f"   æˆåŠŸ: {len(downloaded)} å¼ ")
    print(f"   å¤±è´¥: {len(image_urls) - len(downloaded)} å¼ ")
    print(f"   ä¿å­˜ä½ç½®: {os.path.abspath(SAVE_DIR)}")
    print("=" * 50)
    
    # åˆ—å‡ºæ–‡ä»¶ç±»å‹ç»Ÿè®¡
    if downloaded:
        exts = {}
        for f in downloaded:
            ext = os.path.splitext(f)[1].lower()
            exts[ext] = exts.get(ext, 0) + 1
        print("\nğŸ“Š æ–‡ä»¶ç±»å‹ç»Ÿè®¡:")
        for ext, count in sorted(exts.items(), key=lambda x: -x[1]):
            print(f"   {ext}: {count} å¼ ")
    
    return downloaded

if __name__ == "__main__":
    main()
